---
title: "Clustering and classification"
---

# Clustering and classification

*Tuomas Keski-Kuha 28.11.2021*

------------------------------------------------------

## Data exploration and finding correlations

----------------------------------------------------

*Exercise 2: Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. Details about the Boston dataset can be seen for example here.*

----------------------------------------------------

Let's see the Boston data in the following: 

```{r}
# access a few libraries: 
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(GGally)
library(reshape2)
library(plotly)

# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)

```

**Data set information**

Data is Housing Values in Suburbs of Boston. It has 506 rows and 14 columns. It has some information on the suburbs of Boston (towns).
Here is the [link](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) to the data page, where one can see the full details. 

----------------------------------------------------

*Exercise 3: Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them.*

----------------------------------------------------

Let's visualize the data and take a look of the distributions of the variables and relations between variables. 

```{r}

summary(Boston)

# First lets make a long table from Boston data
melt.boston <- melt(Boston)

# Then plot every variable, so we'll see the distributions of variables
ggplot(data = melt.boston, aes(x = value)) + 
stat_density() + 
facet_wrap(~variable, scales = "free")

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)


```

There is significant variability in the data as some variables are really skewed towards the minimum or the maximum value (most of the values are near or exactly minimum or maximum). There is also few integer variables in the data, which don't fit quite well to the density plot above. Few of the variables seems   normally distributed. 

It is clear that there is really strong correlations (positive and negative) between variables in the data, as correlation plot shows. In the correlation plot the colour and the size of the circles tells about the correlations between variables. 

Quite interestingly the *rad* (radial highway) accessibility) and the *tax* (property tax rate) variables have the biggest positive correlations between the crime rate in towns. Some might think that variables like *dis* (distances to employment centers) and *lstat* (lower status of population(percent)) would have more correlation between crime rate. In the plot we can also see that *rad* and *tax* are heavily correlated so they are almost like one variable. 

We can also see that there seems to be significant variables like: 

- *rad*: correlation between crime rate 
- *tax*: correlation between *rad* and crime rate
- *indus* (proportion of non-retail business, traditional industry): has many significant correlations between other key variable
- *nox* (nitrogen oxides concentration): has many significant correlations between key variables, seems like a variable which is hard to interpret in any reasonable way, at least there is positive correlation between *indus*.
- *age* (proportion of owner-occupied units built prior 1940): old buildings is negatively correlated between distance to centers and positively between *indus* (more older buildings in more industrialized towns)
- *dis*: many significant correlations between other variables, quite logically negatively correlated between *indus* so traditional industry is far from main centers.
- *lstat*: many correlations to other variables, but perhaps not that significant

All in all, hard to gain significant information in this correlation analysis.

------------------------------------------------------

## Data preparing and making training and test datasets

----------------------------------------------------

*Exercise 4: Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set.*

----------------------------------------------------

Let's scale the variables so that the mean = 0 and variance = 1 for all variables in the dataset: 

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summaries of the scaled variables
summary(boston_scaled)
 
```

We can see now that all the variables have negative and positive values around zero. 

In the following section we create a new categorical quantile variable from *crim* variable, so that the new variable has labels *low, med_low, med_high, high* depending the scaled *crim* variable, and all the new variables have 25 % of the data. 

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

After that we make training and test sets from the data, so that we pick randomly 80 % of the rows to the training set and the rest to the test set.

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

set.seed(123)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

----------------------------------------------------

## Classification - Fitting the Linear Discriminant Analysis model

----------------------------------------------------

*Exercise 5: Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot.*

----------------------------------------------------

Let's fit the LDA model to training data and visualize results. 

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

Discrimination analysis seem to fit the data quite well, as plot indicates. Rad variable (index of accessibility to radial highways) and also zn (proportion of residential land zoned for lots over 25,000 sq.ft.) seem to have biggest effect on the classification process. It looks like rad variable gives really good discrimination for higher crime towns. For the lower crim towns it's not that clear at all. In the picture it seems that rad variable separates higher crime towns from the rest. 

Why then the radiation (index of accessibility to radial highways) has so significant effect. It seems not to be obvious that good connections would effect to crime rate that much and others won't (like lower status population, industrialization or distance from employment centers). Would it perhaps be that there is more density of population near highways or something like that, or more population near highways. Anyway *rad* seems to be good indicator for high crime rate. 

----------------------------------------------------

*Exercise 6: Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. *

----------------------------------------------------

In the next phase we save crime gategories from the test set. Also we make predictions from test data, and see how accurate the model is: 

```{r}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)


```

Model's predicting power seems to be good indeed. Just few bigger inaccuracies, where correct class is two classes away. Still all results end up quite near the diagonal. Especially the high class seems to be "easy" to predict. 

----------------------------------------------------

## Clustering - Fitting the K-means model

----------------------------------------------------

*Exercise 7: Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. *

----------------------------------------------------

```{r}
# Load the data again and after that standardize all variables (mean = 0 and variance = 1)
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

Then, we calculate the distances and apply K-means algorithm to make clusters:

```{r}

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

```

Finding the optimal value for the number of clusters: 

```{r}

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

The optimal value is the value where the WCSS (within cluster sum of squares) drops radically (here the optimal number of clusters for K-means seems to be 2).

```{r}

# k-means clustering when k = 2
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston scaled dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

We can see that perhaps *crim, indus, rad, tax, black* variables are best clustering variables (into two clusters), so let's see figures with only those variables. 

```{r}

# lets make a vector containig significant separating variables
a_1 <- c(1, 3, 9, 10, 12)

# plot the Boston scaled dataset with clusters
pairs(boston_scaled[, a_1], col = km$cluster)

```

Presented variables seems to have largest effect on the clustering (into two clusers) as in the plot we can see good separation between the black and pink groups. 

Next, we draw few plot more where one can see these significant variable values by clusters:

```{r}

# let's draw few plots more where significant variable values are presented in a boxplot by clusters
par(mfrow=c(1,3))

boxplot(boston_scaled$rad ~ km$cluster, main = "Rad by clusters")

boxplot(boston_scaled$tax ~ km$cluster, main = "Tax by clusters")

boxplot(boston_scaled$black ~ km$cluster, main = "Black by clusters")

boxplot(boston_scaled$indus ~ km$cluster, main = "Indus by clusters")

boxplot(boston_scaled$crim ~ km$cluster, main = "Crim by clusters")

```

Perhaps one would not become wiser on looking drawn boxplots because many of the picked significant clustering variables has just few instances by cluster (e.g. *crim*, *indus*, *tax*), so one can critisize the separating power here by just one variable. But at least we had more some nice pictures here.  

----------------------------------------------------

## Classification of clusters (Bonus)

----------------------------------------------------

*Bonus exercise: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters?*

----------------------------------------------------


```{r}
# Load the data again and after that standardize all variables (mean = 0 and variance = 1)
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# k-means clustering
km <-kmeans(boston_scaled, centers = 5)

```

Let's add clusters to the data and then apply linear discrinmination analysis to the clusters that we got from K-means clustering: 

```{r}
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, km$cluster)

set.seed(123)

# linear discriminant analysis
lda.fit_2 <- lda(km.cluster ~ ., data = boston_scaled)

# plot the lda results
plot(lda.fit_2, dimen = 2, col = boston_scaled$km.cluster, pch = boston_scaled$km.cluster)
lda.arrows(lda.fit_2, myscale = 1)

```

It seems clear that *chas* and *rad* variables are most influencial linear separators for the five clusters. *Rad* we handled before, it would seem that the highway accessibility makes some difference and separates towns. *Chas* is indicating that the riverside towns are significantly different than the non-riverside towns.

----------------------------------------------------

## 3D-plotting (Super-Bonus)

----------------------------------------------------

```{r}

model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)


plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = ~train$crime)

# k-means clustering
km_2 <-kmeans(matrix_product, centers = 4)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = ~km_2$cluster)

```

Not really sure what was the intention with the latter 3D-plot. I applied K-means (k=4, same amount than in the first plot where predicted) to matrix product data (prediction for the LDA model) intending then to do clustering for predictions. This latter K-means clusterin makes data to look reaaly smoothly separated and there is no outliers or obvious outliers, or data points "inside" other group as in the first one. Now that I look at the second plot, it looks like just the same as you would have if you plot predictions od LDA model.  
