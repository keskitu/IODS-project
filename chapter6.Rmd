---
title: "Analysis of longitudinal data"
---

# Analysis of longitudinal data

*Tuomas Keski-Kuha 12.12.2021*

```{r}
# access libraries: 
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(GGally)
library(reshape2)
library(plotly)
library(tidyverse)
library(rstatix)
library(ggpubr)
library(lme4)

```

----------------------------------------------------

## Graphical Displays and Summary Measure Approach for RATS data

----------------------------------------------------

*Exercise 1: Implement the analyses of Chapter 8 of MABS using the RATS data. (0-7 points: 0-4 points for graphs or analysis results + 0-3 points for their interpretations)*

----------------------------------------------------

### Rats, it's RATS - Graphical Displays of RATS

----------------------------------------------------


Graphical displays of data are almost always useful for exposing patterns in the data, particularly when these are unexpected; this might be of great help in suggesting which class of models might be most sensibly applied in the later more formal analysis.

In the following we have the RATS data which contains rat weight developments in different diets. In the data we have three groups of individual rats that were put on a different diets, and each rat's (16 rats in total) weight in grams was measured repeatedly over a 9-week period. 

Goal of the study was to determine does these diets have different responses on the rat growth. Let's first load the data (long form data) and let's draw a plot where all the information can be seen.  

```{r}

# load the data

# set the working directory to iods project folder
setwd("c:/Tuomas/Opiskelu/Open Data Science/IODS-project")
RATSL <- read.csv(file = "data/RATSL.csv")

RATSL <- within(RATSL, {
  ID <- factor(ID)
  Group <- factor(Group)
})

#  read original RATS data
RATS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/rats.txt", sep  ="\t", header = T)

# changing categorical variables to factors
RATS <- within(RATS, {
  ID <- factor(ID)
  Group <- factor(Group)
})

# draw a plot of the rat data
ggplot(RATSL, aes(x = Time, y = Weight, group = ID)) +
  geom_line(aes(linetype = Group)) +
  scale_x_continuous(name = "Time (days)", breaks = seq(0, 60, 10)) +
  scale_y_continuous(name = "Weight (grams)") +
  theme(legend.position = "top")
```

Comments: 

- First of all starting weights (baseline) are quite different on different rat groups. Group 2 and 3 rats are much heavier than group 1 rats at the start and on the course of the study. 
- In groups 2 and 3 there are fewer rats (4 rats each) than in the first group (8 rats). Also the variety of the baselines for 2 and 3 group rats is larger than in rats in the group 1. 
- It looks like weights of almost all the rats are growing during the study period. 
- At a first glance group 1 rat development seems more stable than in the other groups, where weights seem to grow more rapidly and looks. 

Let's standardize all the rat weights in the next part and let's see if there could be found more variance between the groups. 

```{r}
# Standardise the scores for RATS
RATSL_std <- RATSL %>%
  group_by(Time) %>%
  mutate( stdrats = (Weight - mean(Weight))/sd(Weight) ) %>%   ungroup()

glimpse(RATSL_std)


p1 <- ggplot(RATSL_std, aes(x = Time, y = stdrats, linetype = ID))
p2 <- p1 + geom_line() + scale_linetype_manual(values = rep(1:10, times=4))
p3 <- p2 + facet_grid(. ~ Group, labeller = label_both)
p4 <- p3 + theme_bw() + theme(legend.position = "none")
p5 <- p4 + theme(panel.grid.minor.y = element_blank())
p6 <- p5 + scale_y_continuous(name = "standardized rats")
p6
```

Remarks: 

- All the group 1 rats are packed so tightly so it's not easy to see the differences. Their growth patterns seems to be more similar than in the other groups. 
- In group 2 there is one rat which weight much more than others in that group. Also in group 2 there is one rat which development is quite different than the others (it does not grow that fast than the rest). Actully that rat is really the different one in that group for its growth pattern. 
- In group 3 there's one rat which is growing actually faster than the others.

In the next chapter we calculate summary measures for each group and compare differences between groups. 

----------------------------------------------------

### Summary Measure Approach of RATS

----------------------------------------------------

In the first plot we draw the mean weights and standard deviations around the mean (mean +/- standard deviation).

```{r}

# Number of weeks, baseline (week 0) included
n <- RATSL$Time %>% unique() %>% length()


# Make a summary data:
RATSS <- RATSL %>%
  group_by(Group, Time) %>%
  summarise( mean=mean(Weight), se=sd(Weight)/sqrt(n)) %>%
  ungroup()


p1 <- ggplot(RATSS, aes(x = Time, y = mean, linetype = Group, shape = Group))
p2 <- p1 + geom_line() + scale_linetype_manual(values = c(1,2,3))
p3 <- p2 + geom_point(size=3) + scale_shape_manual(values = c(1,2,3))
p4 <- p3 + geom_errorbar(aes(ymin=mean-se, ymax=mean+se, linetype="1"), width=1.5)
p5 <- p4 + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
p6 <- p5 + theme(legend.position = c(0.9,0.45))
p7 <- p6 + scale_y_continuous(name = "mean(Weight) +/- se(Weight)")
p7


```

In the plot above: 

- Means of the groups seem to develop differently. Groups 2 and 3 seem to grow more rapidly than the group 1 if we look a the mean of the rat weights. 
- Standard deviation around the mean is just telling that there are different weight rats more in groups 2 and 3. It's not that interesting and don't actually tell anything of the variance in growing pattern, if there's any in the groups. 

Next we'll focus on the growth patterns of the rats. Let's calculate the growth percents of all rats at every measuring point and check if there's anything interesting there. After the growth percent calculation, we just study the means of growth percents (from all the measuring points) the groups and plot them into a boxplot.  

```{r}

# Let's use the wide data in which it's perhaps easier to calculate the growth percents

RAT_g <- RATS

# we need a loop (for columns, time points) for calculate the growth percents for all the rats and all
for (i in 4:13) {
  RAT_g[, i] = (RATS[, i]-RATS[, i-1])/RATS[, i-1]*100
  
}

# let's drop the baseline (first weight)
RAT_g <- RAT_g[, -3]

# modify RAT_g to long data
RAT_gL <- gather(RAT_g, key = WD, value = grow, -ID, -Group) %>%
  mutate(Time = as.integer(substr(WD,3,4)))

# Number of weeks, baseline (week 0) included
n <- RAT_gL$Time %>% unique() %>% length()

# Make a summary data:
#RAT_gS <- RAT_gL %>%
#  group_by(Group, Time) %>%
#  summarise( mean=mean(grow), se=sd(grow)/sqrt(n)) %>%
#  ungroup()

# let's calculate the means of the growth percents for all individual rats
RAT_gS1 <- RAT_gL %>%
  group_by(Group, ID) %>%
  summarise( mean=mean(grow) ) %>%
  ungroup()

# plot the mean growth percents
ggplot(RAT_gS1, aes(x = Group, y = mean)) +
  geom_boxplot() +
  stat_summary(fun = "mean", geom = "point", shape=23, size=4, fill = "blue") +
  scale_y_continuous(name = "mean(growth percent)")


```

This plot shows perhaps something interesting in the data, but of course the variability in the group 1 is smaller initially because there's more rats in the group. But here we perhaps see that the mean of the group 2 is different than the other groups if we only focus on the growth pattern. But it's hard to make strong statements based only on these. Perhaps one could also compare growth percents for all time points separately. 

Let's also look at some statistic calculations of the summary measured data. We take the individual rat baseline into linear model analysis to check how relevant factor it is. We'll also fi linear model without baseline as a predictor to see the difference. 

```{r}

# Create a summary data by group and id with mean as the summary variable (ignoring baseline week 0).
RATSL8S <- RATSL %>%
  filter(Time > 0) %>%
  group_by(Group, ID) %>%
  summarise( mean=mean(Weight) ) %>%
  ungroup()

# Add the baseline from the original data as a new variable to the summary data
RATSL8S2 <- RATSL8S %>%
  mutate(baseline = RATS$WD1)

# Fit the linear model with the mean as the response 
fit_1 <- lm(mean ~ Group, data = RATSL8S2)

summary(fit_1)

# Fit the linear model with the mean as the response with baseline as predictor 
fit_2 <- lm(mean ~ baseline + Group, data = RATSL8S2)

summary(fit_2)

# Compute the analysis of variance for the fitted models with anova()
anova(fit_2, fit_1)


```

From these analysis we can see that:

- Without the baseline the group variable seem to be okay predictor for the means, but we can see from the residual standard errors that baseline is much better whem predicting mean. Standard error for baseline model is 10.62 and without it it's 36.66. But still the R-squared measure 0.93 for the model without baseline is not bad at all, so it catches also most of the variability in the data. 
- Predicting mean value of individual rat means is highly correlated between baseline, which is reasonable from the data plots that we saw above (individual lines do not grow so fast or do not variate that much)
- First model (without baseline predictor) also somewhat contains information of the base levels of rats as it's obvious that group levels are different from each other.
- Analysis of the variation (variation is the variation of the residuals) shows also that baseline predictor model is significantly better than the other in its prediction power. RSS is the residual sum of squares.

----------------------------------------------------

##  Linear Mixed Effects Models for Normal Response Variables for BPRS data

----------------------------------------------------

*Exercise 2: Implement the analyses of Chapter 9 of MABS using the BPRS data. (0-8 points: 0-4 points for graphs or analysis results + 0-4 points for their interpretations)*

----------------------------------------------------

In this chapter we'll do several analyses *which all are* linear regression models as title suggests (Linear Mixed Effect Models), but with some added parameters and features. Aim is to have a better model for repetitive data than just basic linear regression, which assumes that all the observations are independent. Here are the four models that we'll apply (in the brackets we shorten the model name): 

- (Basic) Linear regression model (LRM) 
- Random random intercept model (RIM)
- Random random intercept and random slope model (SLO)
- Random intercepts and random slope model with interaction (INT)

The data that we use is the BPRS data 40 male subjects were randomly assigned to one of two treatment groups and each subject was rated on the brief psychiatric rating scale (BPRS) measured before treatment began (week 0) and then at weekly intervals for eight weeks. The BPRS assesses the level of 18 symptom constructs such as hostility, suspiciousness, hallucinations and grandiosity; each of these is rated from one (not present) to seven (extremely severe). The scale is used to evaluate patients suspected of having schizophrenia.

Let's load the BPRS data, take a glimpse, and draw a plot of it. 

```{r}

# load the data

# set the working directory to iods project folder
setwd("c:/Tuomas/Opiskelu/Open Data Science/IODS-project")
BPRSL <- read.csv(file = "data/BPRSL.csv")

BPRSL <- within(BPRSL, {
  treatment <- factor(treatment)
  subject <- factor(subject)
})

# Glimpse the data

glimpse(BPRSL)

# Draw the plot
ggplot(BPRSL, aes(x = week, y = bprs, linetype = subject)) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = c(min(BPRSL$bprs), max(BPRSL$bprs)))
```

Comments of the data:

- Both of the treatment groups have 20 patients each.
- Variability and the overall pattern seems quite similar for each group, peraps for treatment 2 there seems to be a bit more variability and one really high value patient the start. If we don't analyse the just the pattern, but the actual values, it would perhaps be good to drop that observation from the analysis. 
- Also the starting level seems to be similar.  

----------------------------------------------

### Linear regression model (LRM)

----------------------------------------------

First, we try just a simple linear regression model (LRM) on the BPRS-data with *bprs* as response and *week* and *treatment* as explanatory variables. Here we ignore the repeated-measures structure of the data, and assume that observations are all independent knowing that there is same subjects measured multiple times and these observations tend to correlate with each other as we measure same subject. 

The form of (LRM) with two explanatory variables (week (time) $T$ and treatment $X$) is 
$$
Y \sim \beta_0 + \beta_1 T + \beta_2 X + \epsilon
$$
Here the residual term $Y(t, x_i)-y_i$ (the difference between an observed and predicted value - also model error) is normally distributed random variable $\epsilon$ with zero mean and variance $\sigma^2$.

```{r}

# create a regression model BPRS_reg
BPRS_LRM <- lm(bprs ~ week + treatment, data = BPRSL)

# print out a summary of the model
summary(BPRS_LRM)
plot(BPRS_LRM, 1)

```

Commentary: 

- Results of the LRM fit don's seem to be that good, especially the R-squared measure seems pretty low here, so model don't capture the variation in the data that well (if you compare with the similar RATS analysis below for example). 
- Still time variable is significant feature here, and has a significant effect (bprs is heavily going down during the study for the mean at least). And also the intercept value which can be seen as the baseline (constant in the linear regression model).
- Treatment variable on the otherhand don't seem to have significance for the LRM. 
- From the plot we can se that residuals seem to grow with the actual values (indicating non-linearity). That indicates that this model is perhaps not suitable.

----------------------------------------------------

### Random intercept model (RIM)

----------------------------------------------------

The previous model assumes independence of the repeated measures of bprs, and this assumption is highly unlikely. So, no we try something more suitable for repetitive type of study. 

To begin the more formal analysis of the *BPRS* data, we will first fit the random intercept model (RIM) for the same two explanatory variables: *week* and *treatment* Fitting a random intercept model allows the linear regression fit for each subject to differ in intercept from other subjects. So in this model it's possible to have different profiles with symptom development.

The form of (RIM) for the subject $i$ and at the time $t_j$ where $j$ represent the week 
$$
y_{ij} = (\beta_0 + u_i) + \beta_1 t_j + \beta_2 x_i + \epsilon_i
$$
Here the variance of each repeated measurement is ${Var}(y_{ij}) = {Var}(\epsilon_i + u_i) = \sigma^2+u_i^2$ (independent of time), where random effects

- $\epsilon_i \sim N(0,\sigma^2)$ "normal" random effects.
- $u_i \sim N(0,\sigma_u^2)$ is the random effect specific to the subject $i$.

RIM kind of split the random effect in (LRM) to individual random effect which depends of the subject and the basic random effect.

So let's fit this model next: 

```{r}

# Create a random intercept model
BPRS_RIM <- lmer(bprs ~ week + treatment + (1 | subject), data = BPRSL, REML = FALSE)

# Print the summary of the model
summary(BPRS_RIM)

```

Notes: 

- Random effect standard deviance for RIM is not that small comparing with LRM, here variance in total is about 150 as it was 153 for LRM (LRM std. deviance 12.37 so the accuracy is not that amazing)
- RIM model did not critically change the significances of the variables. So the treatment does not not matter in this analysis either. 

----------------------------------------------------

### Random intercept and random slope model (SLO)

----------------------------------------------------

Now we can move on to fit the random intercept and random slope model (SLO) to the data. Fitting a random intercept and random slope model allows the linear regression fits for each individual to differ in intercept but also in slope. This way it is possible to account for the individual differences in the *bprs* development profiles, but also the effect of time.

The form of (SLO) for the subject $i$ and at the time $t_j$ where $j$ represent the week 
$$
y_{ij} = (\beta_0 + u_i) + (\beta_1+v_i) t_j + \beta_2 x_i + \epsilon_{ij}
$$
Here the variance of each repeated measurement is 
$$
{Var}(y_{ij}) = {Var}(u_i + v_it_j +\epsilon_i) = \sigma_u^2+2\sigma_{uv}t_j+\sigma_v^2t_j^2+\sigma^2,
$$
where random effects

- $\epsilon_i \sim N(0,\sigma^2)$ "normal" random effects.
- $u_i \sim N(0,\sigma_u^2)$ is the random effect specific to the subject $i$.
- $v_i \sim N(0,\sigma_v^2)$ allows the linear regression fits for each individual to differ in slope. These are allowed to be correlated with the $u_i$ random intercept effects.

Let's note here that this model allows correlation between residuals (at different time points) of the same individual.  

```{r}

# create a random intercept and random slope model
BPRS_SLO <- lmer(bprs ~ week + treatment + (week | subject), data = BPRSL, REML = FALSE)

# print a summary of the model
summary(BPRS_SLO)

```

Commenting: 

- Log-likelihood is bit better for SLO than RIM,
- Variances of random effects seems to be larger for SLO than for RIM. New slope error term don't capture variance from the model, variance is small for it. 
- Overall change from the RIM model is quite minimal

----------------------------------------------------

### Random intercept and random slope model with interaction (INT)

----------------------------------------------------

Finally, we can fit a random intercept and slope model that allows for a treatment × time interaction.

The form of INT for the subject $i$ and at the time $t_j$ where $j$ represent the week 
$$
y_{ij} = (\beta_0 + u_i) + (\beta_1+v_i) t_j + \beta_2 x_i + \beta_3 x_it_j  + \epsilon_{ij}.
$$

```{r}
# create a random intercept and random slope model with week x treatment interaction
BPRS_INT <- lmer(bprs ~ week * treatment + (week | subject), data = BPRSL, REML = FALSE)

# print a summary of the model
summary(BPRS_INT)

```

Remarks: 

- Log-likelihood is ever so slightly better for INT than SLO, but it is very close call. 
- Variances for the random effects are just a bit lower for INT than SLO. 
- For the fixed effects the new variable week x treatment2 has a bit of a effect for the model. Perhaps for the week x treatment2 term we have a really weak statistical significance, so it indicates that for these two treatments there could be a bit different slopes in general. 

Next let's draw plots for different model fits to compare. 

```{r}

# Create a vector of the fitted values
fitted_RIM <- fitted(BPRS_RIM)

# Create a new column fitted to BPRSL
BPRSL <- BPRSL %>%
  mutate(fitted_RIM)



ggplot(BPRSL, aes(x = week, y = bprs, group = subject)) +
  geom_line(aes(linetype = treatment)) + ggtitle("Original observations") +
  theme(legend.position = "top")
# Draw the plot

ggplot(BPRSL, aes(x = week, y = fitted_RIM, group = subject)) +
  geom_line(aes(linetype = treatment)) + ggtitle("Random intercept fit") +
  theme(legend.position = "top")

# Create a vector of the fitted values
fitted_SLO <- fitted(BPRS_SLO)

# Create a new column fitted to BPRSL
BPRSL <- BPRSL %>%
  mutate(fitted_SLO)

# draw the plot

ggplot(BPRSL, aes(x = week, y = fitted_SLO, group = subject)) +
  geom_line(aes(linetype = treatment)) + ggtitle("Random intercept + slope fit") +
  theme(legend.position = "top")


# Create a vector of the fitted values
fitted_INT <- fitted(BPRS_INT)

# Create a new column fitted to BPRSL
BPRSL <- BPRSL %>%
  mutate(fitted_INT)

# draw the plot

ggplot(BPRSL, aes(x = week, y = fitted_INT, group = subject)) +
  geom_line(aes(linetype = treatment)) + ggtitle("Random intercept + slope + interaction fit") +
  theme(legend.position = "top")


```

Random intercept with random slope with and without interaction is notably different than the random intercept. And perhaps the random intercept with random slope is better for capturing those subject whose symptoms are not decreasing. 

It's quite hard to compare with the actual data because it's so variable and it looks like in the original data it is hard to fit linear model for it. Individual slopes seem to capture the effect that perhaps those who have stronger symptoms will have more "space" to recover and getting better than with those subjects who are better at the start. 

-----------------------------------------------------

### Comments on the exercises and learning

-----------------------------------------------------

This week was really tough to manage and have enough time for exercises. But I managed to squeeze these in time. I think I learned quite a bit this week for these model because it was a struggle. And with better R skills it would have been a lot easier. Intepretation were really challenging this week I think and linear mixed effect models are not that easy to understand as they first seem to be. 

It get me a lot of time to figure out that in BPRS data subjects were not individual so there was two subject = 1 persons. I figured this out just in the end. I had fun still in spite of the challenges. 
